<h1
id="high-volume-fraud-detection-performance-testing-with-hcd-cassandra">High-Volume
Fraud Detection Performance Testing with HCD Cassandra</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>This performance testing initiative demonstrates the capabilities of
HCD (Hyper-Converged DataStax) Cassandra under extreme fraud detection
workloads. Using NoSQLBench, we successfully executed <strong>100
million operations</strong> against a distributed Cassandra cluster,
achieving sustained throughput of <strong>~20,000 reads/sec and ~20,000
writes/sec</strong> while maintaining sub-millisecond latency
characteristics.</p>
<h2 id="infrastructure-architecture">Infrastructure Architecture</h2>
<h3 id="gke-cluster-specifications">GKE Cluster Specifications</h3>
<ul>
<li><strong>Platform</strong>: Google Kubernetes Engine (GKE) in
us-east1 region</li>
<li><strong>Kubernetes Version</strong>: v1.33.5-gke.1080000</li>
<li><strong>Total Nodes</strong>: 10 nodes across 3 availability zones
(us-east1-b/c/d)</li>
</ul>
<h4 id="node-pool-configuration">Node Pool Configuration</h4>
<table>
<thead>
<tr class="header">
<th>Node Pool</th>
<th>Count</th>
<th>vCPUs</th>
<th>Memory</th>
<th>Ephemeral Storage</th>
<th>Role</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Control Plane</td>
<td>3</td>
<td>2</td>
<td>4GB</td>
<td>12GB</td>
<td>Control plane management</td>
</tr>
<tr class="even">
<td>Database</td>
<td>6</td>
<td>16</td>
<td>64GB</td>
<td>21GB</td>
<td>HCD Cassandra nodes</td>
</tr>
<tr class="odd">
<td>Applications</td>
<td>1</td>
<td>16</td>
<td>64GB</td>
<td>28GB</td>
<td>NoSQLBench workload</td>
</tr>
</tbody>
</table>
<p><strong>Total Cluster Resources</strong>: 102 vCPUs, 452GB RAM</p>
<h3 id="hcd-cassandra-deployment">HCD Cassandra Deployment</h3>
<ul>
<li><strong>Node Count</strong>: 6 HCD Cassandra nodes</li>
<li><strong>Replication Strategy</strong>: NetworkTopologyStrategy with
RF=3 (dc-1)</li>
<li><strong>Datacenter</strong>: dc-1 with 6 nodes distributed across 3
availability zones</li>
<li><strong>Storage</strong>: Google Persistent Disk
(pd.csi.storage.gke.io)</li>
</ul>
<h2 id="database-schema-design">Database Schema Design</h2>
<h3 id="table-architecture">Table Architecture</h3>
<h4 id="main-transactions-table-fraud_detection.transactions">1. Main
Transactions Table (<code>fraud_detection.transactions</code>)</h4>
<p><strong>Purpose</strong>: Primary transaction storage with
comprehensive fraud detection attributes</p>
<p><strong>Schema Highlights</strong>: - <strong>Primary Key</strong>:
<code>transaction_id</code> (partition key) - <strong>Data
Types</strong>: 20 columns spanning text, double, int, timestamp types -
<strong>Compaction Strategy</strong>: SizeTieredCompactionStrategy for
high write throughput - <strong>TTL</strong>: 7,776,000 seconds (90
days) - <strong>Compression</strong>: LZ4Compressor with 4KB chunks</p>
<p><strong>Estimated Row Size</strong>: ~350 bytes per row - Text fields
(transaction_id, user_id, merchant_id, etc.): ~180 bytes - Numeric
fields (amount, risk_score, ml_fraud_score, etc.): ~60 bytes<br />
- Timestamp fields: ~16 bytes - Categorical fields (currency,
transaction_type, etc.): ~94 bytes</p>
<p><strong>Sample Data:</strong></p>
<pre><code>transaction_id      | amount     | currency | transaction_type | country_code | city   | device_type | is_fraud | risk_score
--------------------+------------+----------+------------------+--------------+--------+-------------+----------+------------
7667067912213249559 |  8304.5086 |      GBP |         TRANSFER |           DE |  Other |         ATM |    false |        830
9001040773479899721 | 9749.21409 |      CAD |          PAYMENT |           JP |  Other |         POS |     true |        974
1168386195310972831 | 1266.37353 |      USD |         PURCHASE |           US | London |      MOBILE |    false |        126</code></pre>
<h4 id="user-transactions-table-fraud_detection.user_transactions">2.
User Transactions Table
(<code>fraud_detection.user_transactions</code>)</h4>
<p><strong>Purpose</strong>: Time-ordered transaction history per user
for pattern analysis</p>
<p><strong>Schema Highlights</strong>: - <strong>Primary Key</strong>:
<code>user_id</code> (partition key),
<code>transaction_timestamp, transaction_id</code> (clustering keys) -
<strong>Clustering Order</strong>:
<code>transaction_timestamp DESC, transaction_id ASC</code> -
<strong>Compaction Strategy</strong>: TimeWindowCompactionStrategy for
time-series data - <strong>Data Types</strong>: 7 columns optimized for
temporal queries</p>
<p><strong>Estimated Row Size</strong>: ~120 bytes per row -
Primary/clustering keys: ~60 bytes - Transaction metadata: ~60 bytes</p>
<p><strong>Sample Data:</strong></p>
<pre><code>user_id | transaction_timestamp           | transaction_id      | amount     | is_fraud | merchant_id | risk_score
--------+---------------------------------+---------------------+------------+----------+-------------+------------
 544844 | 2024-01-02 02:14:34.623000+0000 | 5030335829473577234 | 5448.90229 |    false |       27242 |        544
 544844 | 2024-01-02 01:45:37.883000+0000 | 5030332423006334831 |  5448.8986 |    false |       27242 |        544
 544844 | 2024-01-02 00:56:10.207000+0000 | 5030332372637010955 | 5448.89855 |    false |       27242 |        544</code></pre>
<h3 id="data-diversity-testing-scope">Data Diversity &amp; Testing
Scope</h3>
<p>The workload incorporated <strong>13 distinct data types</strong>
across both tables: - <strong>Text</strong>: UUIDs, user IDs,
categorical values - <strong>Numeric</strong>: Doubles (amounts, ML
scores), integers (risk scores, velocities)<br />
- <strong>Temporal</strong>: Timestamp fields for transaction timing -
<strong>Geographic</strong>: Country codes, city names, IP addresses -
<strong>Categorical</strong>: Weighted distributions for realistic data
patterns</p>
<h2 id="performance-testing-configuration">Performance Testing
Configuration</h2>
<h3 id="nosqlbench-workload-design">NoSQLBench Workload Design</h3>
<div class="sourceCode" id="cb3"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Test Phases</span><span class="kw">:</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">1. Schema Creation</span><span class="kw">:</span><span class="at"> 3 DDL operations</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">2. Rampup Phase</span><span class="kw">:</span><span class="at"> 100,000 insert operations </span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="fu">3. Main Phase</span><span class="kw">:</span><span class="at"> 100,000,000 mixed read/write operations</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="fu">Operation Mix</span><span class="kw">:</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="kw">-</span><span class="at"> 50% Read Operations (transaction lookups, user history queries)</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="kw">-</span><span class="at"> 50% Write Operations (dual table inserts per transaction)</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="fu">Threading</span><span class="kw">:</span><span class="at"> threads=auto (optimal client connection scaling)</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="fu">Rate Limiting</span><span class="kw">:</span><span class="at"> Uncapped for maximum throughput testing</span></span></code></pre></div>
<h3 id="test-execution-parameters">Test Execution Parameters</h3>
<ul>
<li><strong>Target Operations</strong>: 100 million operations</li>
<li><strong>Actual Records Inserted</strong>: 200 million (100M per
table)</li>
<li><strong>Consistency Level</strong>: LOCAL_QUORUM</li>
<li><strong>Client Auto-scaling</strong>: Leveraged NoSQLBench’s
automatic thread optimization</li>
</ul>
<h2 id="performance-results">Performance Results</h2>
<h3 id="operation-success-metrics">Operation Success Metrics</h3>
<table>
<thead>
<tr class="header">
<th>Phase</th>
<th>Operations</th>
<th>Success Rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Schema</td>
<td>3</td>
<td>100%</td>
</tr>
<tr class="even">
<td>Rampup</td>
<td>100,000</td>
<td>100%</td>
</tr>
<tr class="odd">
<td>Main</td>
<td>100,000,000</td>
<td>100%</td>
</tr>
</tbody>
</table>
<h3 id="latency-performance-main-phase">Latency Performance (Main
Phase)</h3>
<p>Based on NoSQLBench metrics from the main testing phase:</p>
<table>
<thead>
<tr class="header">
<th>Percentile</th>
<th>Latency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>50th (Median)</td>
<td>0.98 milliseconds</td>
</tr>
<tr class="even">
<td>75th</td>
<td>1.10 milliseconds</td>
</tr>
<tr class="odd">
<td>95th</td>
<td>1.32 milliseconds</td>
</tr>
<tr class="even">
<td>98th</td>
<td>1.44 milliseconds</td>
</tr>
<tr class="odd">
<td>99th</td>
<td>1.56 milliseconds</td>
</tr>
<tr class="even">
<td>99.9th</td>
<td>6.87 milliseconds</td>
</tr>
</tbody>
</table>
<p><strong>Additional Metrics:</strong> - <strong>Mean Latency</strong>:
1.03 milliseconds - <strong>Min Latency</strong>: 0.55
milliseconds<br />
- <strong>Max Latency</strong>: 152.12 milliseconds</p>
<p><strong>Key Insight</strong>: 99% of operations completed in under
1.6 milliseconds, with a median response time of ~1 millisecond,
demonstrating excellent performance for high-volume fraud detection
workloads.</p>
<h3 id="throughput-analysis-grafana-metrics">Throughput Analysis
(Grafana Metrics)</h3>
<h4 id="sustained-workload-performance">Sustained Workload
Performance</h4>
<ul>
<li><strong>Write Throughput</strong>: Consistent ~20,000 writes/sec
over 45-minute duration</li>
<li><strong>Read Throughput</strong>: Stable ~20,000 reads/sec
throughout test execution<br />
</li>
<li><strong>Combined Throughput</strong>: 40,000 operations/sec
sustained</li>
</ul>
<p><img src="./Writes_and_reads.png"
alt="Reads and Writes Performance" /> <em>Figure 1: Sustained read/write
throughput showing consistent 20K ops/sec for each operation type over
the entire test duration (14:15 - 14:55)</em></p>
<h4 id="latency-characteristics-under-load">Latency Characteristics
Under Load</h4>
<p><strong>Coordinator Write Latency</strong>: - P50: ~700 microseconds
- P75: ~800 microseconds - P90: ~900 microseconds - P95: ~950
microseconds - P99: ~1,100 microseconds - P99.9: ~1,500 microseconds</p>
<p><strong>Coordinator Read Latency</strong>: - P50: ~800 microseconds -
P75: ~900 microseconds - P90: ~1,200 microseconds - P95: ~1,400
microseconds - P99: ~1,600 microseconds</p>
<p><img src="./write-and-read-latency.png"
alt="Write and Read Latency" /> <em>Figure 2: Coordinator latency
percentiles showing write latencies (top) staying under 1.5ms and read
latencies (bottom) under 2ms throughout the test</em></p>
<h3 id="system-behavior-analysis">System Behavior Analysis</h3>
<h4 id="client-connection-scaling">Client Connection Scaling</h4>
<p>Grafana monitoring showed client connections scaling from 2 to 9
concurrent connections, demonstrating NoSQLBench’s adaptive client
management for optimal resource utilization.</p>
<p><img src="./connected-clients.png" alt="Connected Clients" />
<em>Figure 3: Client connection scaling from 2 to 9 concurrent
connections, showing NoSQLBench’s automatic optimization for maximum
throughput</em></p>
<h4 id="compaction-activity">Compaction Activity</h4>
<p>The Size Tiered Compaction Strategy performed effectively under high
write load: - Periodic compaction spikes reaching 6-8 pending
compactions - Efficient compaction completion preventing accumulation -
Gradual increase in compaction activity correlating with data volume
growth</p>
<p><img src="./compactions.png" alt="Compaction Activity" /> <em>Figure
4: Pending compactions over time, showing healthy compaction behavior
with periodic spikes efficiently processed, reaching peak activity
during the final phase of the 100M operation test</em></p>
<h2 id="key-performance-insights">Key Performance Insights</h2>
<h3 id="sub-millisecond-response-times">1. <strong>Sub-Millisecond
Response Times</strong></h3>
<p>The cluster maintained exceptional latency characteristics with 99%
of operations completing under 1 microsecond, crucial for real-time
fraud detection systems.</p>
<h3 id="linear-scalability">2. <strong>Linear Scalability</strong></h3>
<p>40,000 combined ops/sec throughput with 6 database nodes demonstrates
excellent horizontal scaling capabilities.</p>
<p><strong>Important Note</strong>: The NoSQLBench application was
intentionally resource-constrained with only <strong>4 vCPUs
allocated</strong> (out of 16 available on the n2-standard-16 node).
This deliberate limitation prevented the client from scaling up to put
higher load on Cassandra. The sustained 40K ops/sec throughput
demonstrates that this performance level is sustainable, but
<strong>significantly higher throughput could be achieved</strong> if
the application were given more CPU resources to submit additional
concurrent operations.</p>
<h3 id="operational-stability">3. <strong>Operational
Stability</strong></h3>
<p>45+ minutes of sustained high-throughput testing with zero errors
proves production-readiness for mission-critical fraud detection
workloads.</p>
<h3 id="resource-efficiency">4. <strong>Resource
Efficiency</strong></h3>
<p>Compaction strategies (Size Tiered for main table, Time Window for
user table) provided optimal performance for their respective access
patterns.</p>
<h2 id="technical-architecture-decisions">Technical Architecture
Decisions</h2>
<h3 id="replication-factor-3">Replication Factor: 3</h3>
<ul>
<li>Ensures high availability across 3 availability zones</li>
<li>Balances data durability with write performance</li>
<li>LOCAL_QUORUM consistency provides strong consistency within
datacenter</li>
</ul>
<h3 id="compaction-strategy-selection">Compaction Strategy
Selection</h3>
<ul>
<li><strong>SizeTieredCompactionStrategy</strong>: Chosen for main
transactions table to optimize high-volume random writes</li>
<li><strong>TimeWindowCompactionStrategy</strong>: Selected for
user_transactions to optimize time-series query patterns</li>
</ul>
<h3 id="data-modeling-excellence">Data Modeling Excellence</h3>
<ul>
<li>Partition key design prevents hot partitioning</li>
<li>Clustering keys enable efficient time-range queries</li>
<li>Dual-table architecture supports both transactional and analytical
access patterns</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>This comprehensive performance testing validates HCD Cassandra’s
capability to handle extreme fraud detection workloads at enterprise
scale. The successful execution of 100 million operations with
sub-millisecond latencies and sustained 40K ops/sec throughput
demonstrates readiness for production fraud detection systems requiring
real-time decision making.</p>
<p>The combination of thoughtful schema design, appropriate compaction
strategies, and robust infrastructure provides a solid foundation for
financial services requiring high-performance, low-latency data
processing capabilities.</p>
<hr />
<p><strong>Test Environment</strong>: GKE cluster with 6 HCD Cassandra
nodes (16 vCPU, 64GB RAM each)<br />
<strong>Test Duration</strong>: ~45 minutes<br />
<strong>Total Data Processed</strong>: 200 million records across 2
tables<br />
<strong>Technology Stack</strong>: HCD Cassandra, NoSQLBench,
Kubernetes, Google Cloud Platform</p>
